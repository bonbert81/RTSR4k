{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RTSR4k.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMbNQHmhyvA68wVSUbNwC+F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gemenerik/RTSR4k/blob/master/RTSR4k.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Mas32TsNIQ3",
        "colab_type": "text"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>RTSR4k</h1>\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Lorem ipsum dolor sit amet, consectetur adipiscing elit. Praesent tincidunt pretium enim. Nunc fermentum malesuada nibh non scelerisque. Praesent et erat et ligula iaculis elementum vitae ac odio. Fusce feugiat mollis aliquam. Curabitur tristique dolor ut orci volutpat gravida. Nunc malesuada tempor mauris, eget posuere ante consectetur ac. Ut maximus justo nisi, ut eleifend odio posuere sed. Ut dolor urna, ullamcorper ut turpis sit amet, finibus ultricies massa. Sed sollicitudin nisl id libero ullamcorper suscipit.\n",
        "\n",
        "Nullam auctor leo tellus, nec viverra massa malesuada quis. Nunc euismod nisl euismod, dignissim quam eu, lobortis mauris. Nunc efficitur facilisis viverra. Fusce at arcu non risus lacinia vestibulum. Nunc nec condimentum lorem. Pellentesque laoreet neque et metus luctus, ac finibus massa iaculis. Ut eu pulvinar ex, in dignissim ipsum. Etiam vitae lacinia felis. Praesent ornare dolor eget euismod sollicitudin."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQC765IXPY0E",
        "colab_type": "text"
      },
      "source": [
        "### Pull from GitHub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P20arN6PRcu",
        "colab_type": "code",
        "outputId": "7de46140-80ca-4a2d-aa8c-4de4d7d0fb01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        }
      },
      "source": [
        "!sudo rm -rf RTSR4k\n",
        "!git clone https://github.com/gemenerik/RTSR4k\n",
        "\n",
        "import os\n",
        "os.chdir('RTSR4k/data')\n",
        "os.mkdir('val')\n",
        "os.chdir('..')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'RTSR4k'...\n",
            "remote: Enumerating objects: 93, done.\u001b[K\n",
            "remote: Counting objects: 100% (93/93), done.\u001b[K\n",
            "remote: Compressing objects: 100% (72/72), done.\u001b[K\n",
            "remote: Total 205 (delta 32), reused 71 (delta 15), pack-reused 112\u001b[K\n",
            "Receiving objects: 100% (205/205), 18.51 MiB | 22.89 MiB/s, done.\n",
            "Resolving deltas: 100% (40/40), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxwLEJb4zBh8",
        "colab_type": "text"
      },
      "source": [
        "### Tweakable settings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xZu00nIzDK_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "UPSCALE_FACTOR = 3 #@param {type:\"slider\", min:1, max:5, step:1}\n",
        "GAUSSIAN_BLUR_RADIUS = 0.2 #@param {type:\"slider\", min:0, max:1, step:0.1}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nIABOHU_N96I",
        "colab_type": "text"
      },
      "source": [
        "### Data processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "21nsv-8Ryuxt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "d74cf9fe-7ab8-46e9-e495-6feb4b471dc7"
      },
      "source": [
        "import argparse\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import join\n",
        "\n",
        "from PIL import Image, ImageFilter\n",
        "from torch.utils.data.dataset import Dataset\n",
        "from torchvision.transforms import Compose, CenterCrop, Scale\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def is_image_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in ['.png', '.jpg', '.jpeg', '.JPG', '.JPEG', '.PNG', '.bmp'])\n",
        "\n",
        "\n",
        "def is_video_file(filename):\n",
        "    return any(filename.endswith(extension) for extension in ['.mp4', '.avi', '.mpg', '.mkv', '.wmv', '.flv'])\n",
        "\n",
        "\n",
        "def calculate_valid_crop_size(crop_size, upscale_factor):\n",
        "    return crop_size - (crop_size % upscale_factor)\n",
        "\n",
        "def blur(img):\n",
        "    img2 = img.filter(ImageFilter.GaussianBlur(GAUSSIAN_BLUR_RADIUS))\n",
        "    return img2\n",
        "\n",
        "\n",
        "def input_transform(crop_size, upscale_factor):\n",
        "    return Compose([\n",
        "        CenterCrop(crop_size),\n",
        "        Scale(crop_size // upscale_factor, interpolation=Image.BICUBIC), blur\n",
        "    ])\n",
        "\n",
        "\n",
        "def target_transform(crop_size):\n",
        "    return Compose([\n",
        "        CenterCrop(crop_size)\n",
        "    ])\n",
        "\n",
        "\n",
        "class DatasetFromFolder(Dataset):\n",
        "    def __init__(self, dataset_dir, upscale_factor, input_transform=None, target_transform=None):\n",
        "        super(DatasetFromFolder, self).__init__()\n",
        "        self.image_dir = dataset_dir + '/scaling_factor_' + str(upscale_factor) + '/data'\n",
        "        self.target_dir = dataset_dir + '/scaling_factor_' + str(upscale_factor) + '/target'\n",
        "        self.image_filenames = [join(self.image_dir, x) for x in listdir(self.image_dir) if is_image_file(x)]\n",
        "        self.target_filenames = [join(self.target_dir, x) for x in listdir(self.target_dir) if is_image_file(x)]\n",
        "        self.input_transform = input_transform\n",
        "        self.target_transform = target_transform\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image, _, _ = Image.open(self.image_filenames[index]).convert('YCbCr').split()\n",
        "        target, _, _ = Image.open(self.target_filenames[index]).convert('YCbCr').split()\n",
        "        if self.input_transform:\n",
        "            image = self.input_transform(image)\n",
        "        if self.target_transform:\n",
        "            target = self.target_transform(target)\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_filenames)\n",
        "\n",
        "\n",
        "def generate_dataset(data_type, upscale_factor, data_target):\n",
        "    images_name = [x for x in listdir('data/' + data_type) if is_image_file(x)]\n",
        "    crop_size = calculate_valid_crop_size(256, upscale_factor)\n",
        "    lr_transform = input_transform(crop_size, upscale_factor)\n",
        "    hr_transform = target_transform(crop_size)\n",
        "\n",
        "    root = 'data/' + data_target\n",
        "    if not os.path.exists(root):\n",
        "        os.makedirs(root)\n",
        "    path = root + '/scaling_factor_' + str(upscale_factor)\n",
        "    if not os.path.exists(path):\n",
        "        os.makedirs(path)\n",
        "    image_path = path + '/data'\n",
        "    if not os.path.exists(image_path):\n",
        "        os.makedirs(image_path)\n",
        "    target_path = path + '/target'\n",
        "    if not os.path.exists(target_path):\n",
        "        os.makedirs(target_path)\n",
        "\n",
        "    for image_name in tqdm(images_name, desc='generate ' + data_type + ' dataset with upscale factor = '\n",
        "            + str(upscale_factor) + ' from dataset'):\n",
        "        image = Image.open('data/' + data_type + '/' + image_name)\n",
        "        target = image.copy()\n",
        "        image = lr_transform(image)\n",
        "        target = hr_transform(target)\n",
        "\n",
        "        image.save(image_path + '/' + image_name)\n",
        "        target.save(target_path + '/' + image_name)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    generate_dataset(data_type='original_data/train', upscale_factor=UPSCALE_FACTOR, data_target='train')\n",
        "    generate_dataset(data_type='original_data/test/Set14', upscale_factor=UPSCALE_FACTOR, data_target='test')\n",
        "    generate_dataset(data_type='val', upscale_factor=UPSCALE_FACTOR, data_target='val')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py:220: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n",
            "  \"please use transforms.Resize instead.\")\n",
            "generate original_data/train dataset with upscale factor = 3 from dataset: 100%|██████████| 91/91 [00:00<00:00, 514.42it/s]\n",
            "generate original_data/test/Set14 dataset with upscale factor = 3 from dataset: 100%|██████████| 14/14 [00:00<00:00, 34.40it/s]\n",
            "generate val dataset with upscale factor = 3 from dataset: 0it [00:00, ?it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3grncVwQmpK",
        "colab_type": "text"
      },
      "source": [
        "### Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "omkzo-EGQpVu",
        "colab_type": "code",
        "outputId": "ab445263-0e1c-462d-fec5-73559f35f39c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 830
        }
      },
      "source": [
        "from model import Net\n",
        "from torchvision import transforms\n",
        "from data_utils import DatasetFromFolder\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim\n",
        "import pylab\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    net = Net(upscale_factor=UPSCALE_FACTOR)\n",
        "    print(net)\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    print('Running on', device)\n",
        "    if device == 'cuda':\n",
        "        net.cuda()\n",
        "    transform = transforms.Compose([\n",
        "    # you can add other transformations in this list\n",
        "    transforms.ToTensor()\n",
        "    ])\n",
        "    \n",
        "    # trainset = torchvision.datasets.ImageFolder(root = './data/train/SRF_3', transform=transforms.ToTensor(),\n",
        "    #                                  target_transform=None)\n",
        "    \n",
        "    trainset = DatasetFromFolder('data/train', upscale_factor=UPSCALE_FACTOR, input_transform=transforms.ToTensor(),\n",
        "                                  target_transform=transforms.ToTensor())\n",
        "    \n",
        "    testset = DatasetFromFolder('data/val', upscale_factor=UPSCALE_FACTOR, input_transform=transforms.ToTensor(),\n",
        "                                target_transform=transforms.ToTensor())\n",
        "    \n",
        "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "    \n",
        "    # testset = torchvision.datasets.ImageFolder(root = './data/val/SRF_3', transform=transform,\n",
        "    #                                  target_transform=None)\n",
        "    \n",
        "    testloader = torch.utils.data.DataLoader(testset, batch_size=4,\n",
        "                                         shuffle=False, num_workers=2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = torch.optim.Adam(net.parameters(), lr=10e-3)\n",
        "\n",
        "    \" train net \"\n",
        "    epochs = []\n",
        "    losses = []\n",
        "    plt.ion()\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    ax.set_ylabel('total loss')\n",
        "    ax.set_xlabel('epoch')\n",
        "    Ln, = ax.plot([0],[1])\n",
        "    pylab.show()\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
        "\n",
        "    for epoch in range(30):  # loop over the dataset multiple times\n",
        "    \n",
        "        running_loss = 0.0\n",
        "        for i, data in enumerate(trainloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "    \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "    \n",
        "            # forward + backward + optimize\n",
        "            outputs = net(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss += loss.item()\n",
        "            \n",
        "        print('[%d, %5d] total loss: %.3f' %\n",
        "              (epoch + 1, i + 1, running_loss,))\n",
        "        epochs.append(epoch+1)\n",
        "        losses.append(running_loss)\n",
        "        Ln.set_ydata(losses)\n",
        "        Ln.set_xdata(epochs)\n",
        "        ax.set_xlim(1,epoch+1)\n",
        "        ax.set_ylim(0,max(losses))\n",
        "        fig.canvas.draw()\n",
        "        plt.show()\n",
        "        plt.pause(0.1)\n",
        "        \n",
        "        running_loss = 0.0\n",
        "\n",
        "        scheduler.step()\n",
        "        print('lr: ' + str(scheduler.get_lr()))\n",
        "\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    \" save \"\n",
        "    PATH = './Trained.pth'\n",
        "    torch.save(net.state_dict(), PATH)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Net(\n",
            "  (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
            "  (conv2): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (conv3): Conv2d(32, 9, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "  (pixel_shuffle): PixelShuffle(upscale_factor=3)\n",
            ")\n",
            "Running on cuda\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "[1,    23] total loss: 1.182\n",
            "lr: [0.01]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:87: UserWarning: Attempting to set identical left == right == 1 results in singular transformations; automatically expanding.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "[2,    23] total loss: 0.213\n",
            "lr: [0.01]\n",
            "[3,    23] total loss: 0.136\n",
            "lr: [0.01]\n",
            "[4,    23] total loss: 0.112\n",
            "lr: [0.01]\n",
            "[5,    23] total loss: 0.096\n",
            "lr: [0.01]\n",
            "[6,    23] total loss: 0.087\n",
            "lr: [0.01]\n",
            "[7,    23] total loss: 0.112\n",
            "lr: [0.01]\n",
            "[8,    23] total loss: 0.159\n",
            "lr: [0.01]\n",
            "[9,    23] total loss: 0.152\n",
            "lr: [0.01]\n",
            "[10,    23] total loss: 0.109\n",
            "lr: [0.0025]\n",
            "[11,    23] total loss: 0.070\n",
            "lr: [0.005]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0VbohgjdYB_",
        "colab_type": "text"
      },
      "source": [
        "### Test image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yI4gm2K-dWzl",
        "colab_type": "code",
        "outputId": "8912ede3-04d7-4ffc-9205-0511fa5dbe25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        }
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "from model import Net\n",
        "\n",
        "from torchvision import transforms\n",
        "from data_utils import DatasetFromFolder\n",
        "\n",
        "trainset = DatasetFromFolder('data/train', upscale_factor=UPSCALE_FACTOR, input_transform=transforms.ToTensor(),\n",
        "                                  target_transform=transforms.ToTensor())\n",
        "\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n",
        "                                          shuffle=True, num_workers=2)\n",
        "\n",
        "\" load net \"\n",
        "PATH = 'Trained.pth'\n",
        "net = Net(UPSCALE_FACTOR)\n",
        "net.load_state_dict(torch.load(PATH))\n",
        "\n",
        "i = UPSCALE_FACTOR\n",
        "path = 'data/test/scaling_factor_%s/' %i\n",
        "print(path)\n",
        "image_name = 'baboon.png'\n",
        "\n",
        "\n",
        "img = Image.open(path + '/data/' + image_name).convert('YCbCr')\n",
        "y, cb, cr = img.split()\n",
        "image = Variable(ToTensor()(y)).view(1, -1, y.size[1], y.size[0])\n",
        "\n",
        "inputs, target = next(iter(trainloader))\n",
        "pic = inputs.numpy()\n",
        "output = net(inputs)\n",
        "\n",
        "\n",
        "out = net(image)\n",
        "out_img_y = out.data[0].numpy()\n",
        "out_img_y *= 255.0\n",
        "out_img_y = out_img_y.clip(0, 255)\n",
        "out_img_y = Image.fromarray(np.uint8(out_img_y[0]), mode='L')\n",
        "out_img_cb = cb.resize(out_img_y.size, Image.BICUBIC)\n",
        "out_img_cr = cr.resize(out_img_y.size, Image.BICUBIC)\n",
        "super_res_image = Image.merge('YCbCr', [out_img_y, out_img_cb, out_img_cr]).convert('RGB')\n",
        "super_res_image.save('test.jpg')\n",
        "\n",
        "low_res_image = Image.open(path + 'data/' + image_name)\n",
        "display(low_res_image)\n",
        "display(super_res_image)\n",
        "high_res_image = Image.open(path + 'target/' + image_name)\n",
        "display(high_res_image)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-fcca83310ab9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mPATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'Trained.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mnet\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mUPSCALE_FACTOR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUPSCALE_FACTOR\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    523\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'encoding'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_reader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[0;34m(name_or_buffer, mode)\u001b[0m\n\u001b[1;32m    210\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 212\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    213\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'w'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_open_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Trained.pth'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvRakjhX302E",
        "colab_type": "text"
      },
      "source": [
        "##### PSNR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ngdTjzTr30Cg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "db49dfc7-1444-4575-f1d8-d70d9c004a97"
      },
      "source": [
        "from numpy import mean\n",
        "from math import log10, sqrt\n",
        "from cv2 import imread\n",
        "original = imread(path + 'target/' + image_name)\n",
        "contrast = imread('test.jpg',1)\n",
        "def psnr(img1, img2):\n",
        "    mse = mean( (img1 - img2) ** 2 )\n",
        "    if mse == 0:\n",
        "      return 100\n",
        "    PIXEL_MAX = 255.0\n",
        "    return 20 * log10(PIXEL_MAX / sqrt(mse))\n",
        "\n",
        "d=psnr(original,contrast)\n",
        "print(d)"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "34.423016816177594\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}